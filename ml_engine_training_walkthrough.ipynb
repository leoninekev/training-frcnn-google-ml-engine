{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ml-engine_training_walkthrough.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leoninekev/training-frcnn-google-ml-engine/blob/master/ml_engine_training_walkthrough.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaMVQ59nRP8a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d56b716f-76f6-4fcb-a936-478e221df3d0"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KH6-APjfYZ61",
        "colab_type": "text"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This tutorial shows how to train a neural network on AI Platform\n",
        "using the Keras sequential API and how to serve predictions from that\n",
        "model.\n",
        "\n",
        "Keras is a high-level API for building and training deep learning models.\n",
        "[tf.keras](https://www.tensorflow.org/guide/keras) is TensorFlowâ€™s\n",
        "implementation of this API.\n",
        "\n",
        "The first two parts of the tutorial walk through training a model on Cloud\n",
        "AI Platform using prewritten Keras code, deploying the trained model to\n",
        "AI Platform, and serving online predictions from the deployed model.\n",
        "\n",
        "The last part of the tutorial digs into the training code used for this model and ensuring it's compatible with AI Platform. To learn more about building\n",
        "machine learning models in Keras more generally, read [TensorFlow's Keras\n",
        "tutorials](https://www.tensorflow.org/tutorials/keras)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGCL9ejmayR3",
        "colab_type": "text"
      },
      "source": [
        "### Set up your GCP project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a GCP project.](https://console.cloud.google.com/cloud-resource-manager)\n",
        "\n",
        "2. [Make sure that billing is enabled for your project.](https://cloud.google.com/billing/docs/how-to/modify-project)\n",
        "\n",
        "3. [Enable the AI Platform (\"Cloud Machine Learning Engine\") and Compute Engine APIs.](https://console.cloud.google.com/flows/enableapi?apiid=ml.googleapis.com,compute_component)\n",
        "\n",
        "4. Enter your project ID in the cell below. Then run the  cell to make sure the\n",
        "Cloud SDK uses the right project for all the commands in this notebook.\n",
        "\n",
        "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ieeq-5lYpbt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "9c9a677a-8c7d-43b8-ab53-ab8932a6349b"
      },
      "source": [
        "PROJECT_ID = \"nifty-episode-231612\" #@param {type:\"string\"}\n",
        "! gcloud config set project $PROJECT_ID"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n",
            "\n",
            "\n",
            "To take a quick anonymous survey, run:\n",
            "  $ gcloud alpha survey\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86ZNT2sJYohG",
        "colab_type": "text"
      },
      "source": [
        "**If you are using Colab**, run the cell below and follow the instructions\n",
        "when prompted to authenticate your account via oAuth.\n",
        "\n",
        "**Otherwise**, follow these steps:\n",
        "\n",
        "1. In the GCP Console, go to the [**Create service account key**\n",
        "   page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
        "\n",
        "2. From the **Service account** drop-down list, select **New service account**.\n",
        "\n",
        "3. In the **Service account name** field, enter a name.\n",
        "\n",
        "4. From the **Role** drop-down list, select\n",
        "   **Machine Learning Engine > AI Platform Admin** and\n",
        "   **Storage > Storage Object Admin**.\n",
        "\n",
        "5. Click *Create*. A JSON file that contains your key downloads to your\n",
        "local environment.\n",
        "\n",
        "6. Enter the path to your service account key as the\n",
        "`GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T02KpSxYY1_9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "fa5dc610-5eb4-4a51-a5fb-10ead7b55cf0"
      },
      "source": [
        "import sys\n",
        "\n",
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "  from google.colab import auth as google_auth\n",
        "  google_auth.authenticate_user()\n",
        "\n",
        "# If you are running this notebook locally, replace the string below with the\n",
        "# path to your service account key and run this cell to authenticate your GCP\n",
        "# account.\n",
        "else:\n",
        "  %env GOOGLE_APPLICATION_CREDENTIALS ''\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0726 04:51:05.403162 140558765889408 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3rmHKEhjbkr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "199ee792-58bf-4fd7-b8bc-41299374a3d5"
      },
      "source": [
        "!gcloud config list"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[component_manager]\n",
            "disable_update_check = True\n",
            "[core]\n",
            "account = quantumbisht@gmail.com\n",
            "project = nifty-episode-231612\n",
            "\n",
            "Your active configuration is: [default]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xc0QJmDGY6_g",
        "colab_type": "text"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "When you submit a training job using the Cloud SDK, you upload a Python package\n",
        "containing your training code to a Cloud Storage bucket. AI Platform runs\n",
        "the code from this package. In this tutorial, AI Platform also saves the\n",
        "trained model that results from your job in the same bucket. You can then\n",
        "create an AI Platform model version based on this output in order to serve\n",
        "online predictions.\n",
        "\n",
        "Set the name of your Cloud Storage bucket below. It must be unique across all\n",
        "Cloud Storage buckets. \n",
        "\n",
        "You may also change the `REGION` variable, which is used for operations\n",
        "throughout the rest of this notebook. Make sure to [choose a region where Cloud\n",
        "AI Platform services are\n",
        "available](https://cloud.google.com/ml-engine/docs/tensorflow/regions). You may\n",
        "not use a Multi-Regional Storage bucket for training with AI Platform."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPo3wYUTY-th",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUCKET_NAME = \"nifty-episode-231612-mlengine\" #@param {type:\"string\"}\n",
        "REGION = \"asia-east1\" #@param {type:\"string\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgdZi_1lbBVZ",
        "colab_type": "text"
      },
      "source": [
        "**Finally, validate access to your Cloud Storage bucket by examining its contents:**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0HC0cSpbKAf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "8afd23e7-3e4e-4a90-845b-294a2daa6592"
      },
      "source": [
        "! gsutil ls -al gs://$BUCKET_NAME"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 209581009  2019-06-24T12:45:15Z  gs://nifty-episode-231612-mlengine/flask_app_git_140619.zip#1561380315059208  metageneration=1\n",
            "      1136  2019-03-14T10:52:42Z  gs://nifty-episode-231612-mlengine/img_gcloudpath.txt#1552560762918268  metageneration=1\n",
            "       885  2019-05-06T12:32:48Z  gs://nifty-episode-231612-mlengine/my_job_filesconfig.pickle#1557145968875019  metageneration=1\n",
            "     10925  2019-03-20T12:29:13Z  gs://nifty-episode-231612-mlengine/preprocessing_test.py#1553084953044119  metageneration=1\n",
            "  87552713  2019-07-24T12:21:59Z  gs://nifty-episode-231612-mlengine/train_on_gcloud_bilkul_final.zip#1563970919338895  metageneration=1\n",
            "     32636  2019-03-19T12:12:00Z  gs://nifty-episode-231612-mlengine/yeaah.jpg#1552997520104445  metageneration=1\n",
            "                                 gs://nifty-episode-231612-mlengine/cloud_test_package/\n",
            "                                 gs://nifty-episode-231612-mlengine/cloud_test_package_2/\n",
            "                                 gs://nifty-episode-231612-mlengine/dev/\n",
            "                                 gs://nifty-episode-231612-mlengine/food_data/\n",
            "                                 gs://nifty-episode-231612-mlengine/keras-frcnn/\n",
            "                                 gs://nifty-episode-231612-mlengine/keras-frcnn_firebase_database_fetching/\n",
            "                                 gs://nifty-episode-231612-mlengine/my_job_files/\n",
            "                                 gs://nifty-episode-231612-mlengine/test_job/\n",
            "                                 gs://nifty-episode-231612-mlengine/test_job_beta1/\n",
            "                                 gs://nifty-episode-231612-mlengine/test_job_kaafi_late/\n",
            "                                 gs://nifty-episode-231612-mlengine/test_job_with_txtfileData6/\n",
            "                                 gs://nifty-episode-231612-mlengine/train_on_gcloud/\n",
            "                                 gs://nifty-episode-231612-mlengine/training_data_and_annotations_for_cloud_060519/\n",
            "                                 gs://nifty-episode-231612-mlengine/training_scripts_collection/\n",
            "TOTAL: 6 objects, 297179304 bytes (283.41 MiB)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdFhraVLbU2j",
        "colab_type": "text"
      },
      "source": [
        "##1. Training in AI Platform\n",
        "\n",
        "This section of the tutorial walks you through submitting a training job to Cloud\n",
        "AI Platform. This job runs sample code that uses Keras to train a deep neural\n",
        "network on the United States Census data. It outputs the trained model as a\n",
        "[TensorFlow SavedModel\n",
        "directory](https://www.tensorflow.org/guide/saved_model#save_and_restore_models)\n",
        "in your Cloud Storage bucket.\n",
        "\n",
        "### Get training code and dependencies\n",
        "\n",
        "Run the following cell to \n",
        "* First, download the training code.\n",
        "\n",
        "* install Python dependencies needed to train the model locally. WBut hen you run the training job in AI Platform, dependencies are preinstalled based on the [runtime version](https://cloud.google.com/ml-engine/docs/tensorflow/runtime-version-list) you choose.\n",
        "* change the notebook's working directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjPizllyRgYE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "5424aa23-33fb-4b49-e7b4-1a42be560ae1"
      },
      "source": [
        "!git clone https://github.com/leoninekev/training-frcnn-google-ml-engine.git\n",
        "\n",
        "! pip install -r requirements.txt\n",
        "\n",
        "# Set the working directory to the sample code directory\n",
        "%cd training-frcnn-google-ml-engine/move_to_cloudshell/"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'training-frcnn-google-ml-engine'...\n",
            "remote: Enumerating objects: 686, done.\u001b[K\n",
            "remote: Counting objects: 100% (686/686), done.\u001b[K\n",
            "remote: Compressing objects: 100% (349/349), done.\u001b[K\n",
            "remote: Total 686 (delta 340), reused 672 (delta 331), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (686/686), 35.21 MiB | 35.80 MiB/s, done.\n",
            "Resolving deltas: 100% (340/340), done.\n",
            "/content/training-frcnn-google-ml-engine/move_to_cloudshell\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57YPYIDaWafj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "ab506ec0-f4a0-44d8-ac30-a998338dbbf8"
      },
      "source": [
        "! ls -pR"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".:\n",
            "annotations.txt  setup.py  trainer/\n",
            "\n",
            "./trainer:\n",
            "config.py\t\t    __init__.py     RoiPoolingConv.py\t   vgg.py\n",
            "data_augment.py\t\t    losses.py\t    simple_parser_pkl.py\n",
            "data_generators.py\t    resnet.py\t    simple_parser_text.py\n",
            "FixedBatchNormalization.py  roi_helpers.py  task.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lo_FAgDrcZ0-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zy_eIZ2dT_d",
        "colab_type": "text"
      },
      "source": [
        "### Train your model using AI Platform\n",
        "\n",
        "Next, submit a training job to AI Platform. This runs the training module\n",
        "in the cloud and exports the training package and trained model to Cloud Storage.\n",
        "\n",
        "Proceed as follows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4TZHpJR63CC",
        "colab_type": "text"
      },
      "source": [
        "* Navigate to trainer/ directory to modify bucket path, model name, config name in task.py in accordance with your gcp service account. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXWqHbCftckM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "7da8965e-c3f4-4eae-8ebf-a01b57837a93"
      },
      "source": [
        "%cd trainer\n",
        "!ls"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/training-frcnn-google-ml-engine/move_to_cloudshell/trainer\n",
            "config.py\t\t    __init__.py     RoiPoolingConv.py\t   vgg.py\n",
            "data_augment.py\t\t    losses.py\t    simple_parser_pkl.py\n",
            "data_generators.py\t    resnet.py\t    simple_parser_text.py\n",
            "FixedBatchNormalization.py  roi_helpers.py  task.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxnUVVzX6wRB",
        "colab_type": "text"
      },
      "source": [
        "*  Run **%pycat task.py** (this draws a pop displayinf content of task.py)\n",
        "* Copy all code to local Python IDE and edit the default arguments values in parsers for\n",
        " * **--path**\n",
        " * **--config_filename** \n",
        " * **--output_weight_path** \n",
        " * **--bucket_path**\n",
        "\n",
        "  and name for **model_weights** before saving.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NdsK5Kit87F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%pycat task.py\n",
        "#copy the code from popup, paste it to a python IDLE locally, edit it and again copy the whole post edit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aU7jVz5Z7BMI",
        "colab_type": "text"
      },
      "source": [
        "* Copy the edited code from local IDE in following colab cell beneath the command: **%%writefile task.py**\n",
        "and run the cell - The new edits will be overwritten to a new task.py file(you may also save a new task_file.py, and later delete the older task.py)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFd1avzJv1zn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cc075123-bf8a-47e0-a234-d06ba7411691"
      },
      "source": [
        "%%writefile task.py\n",
        "from __future__ import division\n",
        "import random\n",
        "import pprint\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "from optparse import OptionParser\n",
        "import pickle\n",
        "\n",
        "from tensorflow.python.lib.io import file_io\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.optimizers import Adam, SGD, RMSprop\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "\n",
        "import config, data_generators\n",
        "import losses as losses\n",
        "import roi_helpers\n",
        "from keras.utils import generic_utils\n",
        "\n",
        "sys.setrecursionlimit(40000)\n",
        "\n",
        "parser = OptionParser()\n",
        "\n",
        "parser.add_option(\"-p\", \"--path\", dest=\"train_path\", help=\"Path to training data(annotation.txt file).\",default=\"gs://nifty-episode-231612-mlengine/training_data_and_annotations_for_cloud_060519/annotations.txt\")# /data.pickle -- for pickled annotations \n",
        "parser.add_option(\"-o\", \"--parser\", dest=\"parser\", help=\"Parser to use. One of simple_text or simple_pickle\",\n",
        "                  default=\"simple\")# simple_pick --for simple_parser_pkl\n",
        "\n",
        "parser.add_option(\"-n\", \"--num_rois\", type=\"int\", dest=\"num_rois\", help=\"Number of RoIs to process at once.\", default=32)\n",
        "parser.add_option(\"--network\", dest=\"network\", help=\"Base network to use. Supports vgg or resnet50.\", default='resnet50')\n",
        "parser.add_option(\"--hf\", dest=\"horizontal_flips\", help=\"Augment with horizontal flips in training. (Default=false).\", action=\"store_true\", default=False)\n",
        "parser.add_option(\"--vf\", dest=\"vertical_flips\", help=\"Augment with vertical flips in training. (Default=false).\", action=\"store_true\", default=False)\n",
        "parser.add_option(\"--rot\", \"--rot_90\", dest=\"rot_90\", help=\"Augment with 90 degree rotations in training. (Default=false).\",\n",
        "                                  action=\"store_true\", default=False)\n",
        "parser.add_option(\"--num_epochs\", type=\"int\", dest=\"num_epochs\", help=\"Number of epochs.\", default=1)# deafult=1 --for test\n",
        "parser.add_option(\"--config_filename\", dest=\"config_filename\", help=\n",
        "                                \"Location to store all the metadata related to the training (to be used when testing).\",\n",
        "                                default=\"config_new.pickle\")\n",
        "parser.add_option(\"--output_weight_path\", dest=\"output_weight_path\", help=\"Output path for weights.\",default='gs://nifty-episode-231612-mlengine/my_job_files/')\n",
        "parser.add_option(\"--input_weight_path\", dest=\"input_weight_path\", help=\"Input path for weights. If not specified, will try to load default weights provided by keras.\",\n",
        "                  default='https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
        "parser.add_option(\"--bucket_path\", dest=\"bucket_path\", help=\"bucket path for stroing weights & configs\",  default='gs://nifty-episode-231612-mlengine/my_job_files/')\n",
        "\n",
        "(options, args) = parser.parse_args()\n",
        "\n",
        "if not options.train_path:# if filename is not given\n",
        "        parser.error('Error: path to training data must be specified. Pass --path to command line')\n",
        "\n",
        "if options.parser == 'simple':\n",
        "        from simple_parser_text import get_data\n",
        "elif options.parser == 'simple_pick':\n",
        "        from simple_parser_pkl import get_data\n",
        "else:\n",
        "        raise ValueError(\"Command line option parser must be one of 'pascal_voc' or 'simple'\")\n",
        "\n",
        "# pass the settings from the command line, and persist them in the config object\n",
        "C = config.Config()\n",
        "\n",
        "C.use_horizontal_flips = bool(options.horizontal_flips)\n",
        "C.use_vertical_flips = bool(options.vertical_flips)\n",
        "C.rot_90 = bool(options.rot_90)\n",
        "\n",
        "C.model_path = options.output_weight_path\n",
        "C.num_rois = int(options.num_rois)\n",
        "\n",
        "if options.network == 'vgg':\n",
        "        C.network = 'vgg'\n",
        "        from keras_frcnn import vgg as nn\n",
        "elif options.network == 'resnet50':\n",
        "        from keras_frcnn import resnet as nn\n",
        "        C.network = 'resnet50'\n",
        "else:\n",
        "        print('Not a valid model')\n",
        "        raise ValueError\n",
        "\n",
        "\n",
        "# check if weight path was passed via command line\n",
        "if options.input_weight_path:\n",
        "        C.base_net_weights = options.input_weight_path\n",
        "else:\n",
        "        # set the path to weights based on backend and model\n",
        "        C.base_net_weights = nn.get_weight_path()# 'resnet50_weights_th_dim_ordering_th_kernels_notop.h5'\n",
        "\n",
        "all_imgs, classes_count, class_mapping = get_data(options.train_path)\n",
        "\n",
        "if 'bg' not in classes_count:\n",
        "        classes_count['bg'] = 0\n",
        "        class_mapping['bg'] = len(class_mapping)\n",
        "\n",
        "C.class_mapping = class_mapping\n",
        "\n",
        "inv_map = {v: k for k, v in class_mapping.items()}\n",
        "\n",
        "print('Training images per class:')\n",
        "pprint.pprint(classes_count)\n",
        "print('Num classes (including bg) = {}'.format(len(classes_count)))\n",
        "\n",
        "config_output_filename = options.bucket_path + options.config_filename# gs://input-your-bucket-name/train_on_gcloud/my_job_files/config.pickle\n",
        "\n",
        "def new_open(name, mode, buffering=-1):# to open & load files from gcloud storage\n",
        "        return file_io.FileIO(name, mode)\n",
        "\n",
        "\n",
        "with new_open(config_output_filename, 'wb') as config_f:\n",
        "        pickle.dump(C,config_f, protocol=2)# dumps config.pickle(compatible for python 2) in gcloud bucket\n",
        "        print('Config has been written to {}, and can be loaded when testing to ensure correct results'.format(config_output_filename))\n",
        "\n",
        "random.shuffle(all_imgs)\n",
        "\n",
        "num_imgs = len(all_imgs)\n",
        "\n",
        "train_imgs = [s for s in all_imgs if s['imageset'] == 'trainval']\n",
        "val_imgs = [s for s in all_imgs if s['imageset'] == 'test']\n",
        "\n",
        "print('Num train samples {}'.format(len(train_imgs)))\n",
        "print('Num val samples {}'.format(len(val_imgs)))\n",
        "\n",
        "\n",
        "data_gen_train = data_generators.get_anchor_gt(train_imgs, classes_count, C, nn.get_img_output_length, K.image_dim_ordering(), mode='train')\n",
        "data_gen_val = data_generators.get_anchor_gt(val_imgs, classes_count, C, nn.get_img_output_length,K.image_dim_ordering(), mode='val')\n",
        "\n",
        "if K.image_dim_ordering() == 'th':\n",
        "        input_shape_img = (3, None, None)\n",
        "else:\n",
        "        input_shape_img = (None, None, 3)\n",
        "\n",
        "img_input = Input(shape=input_shape_img)\n",
        "roi_input = Input(shape=(None, 4))\n",
        "\n",
        "# define the base network (resnet here, can be VGG, Inception, etc)\n",
        "shared_layers = nn.nn_base(img_input, trainable=True)\n",
        "\n",
        "# define the RPN, built on the base layers\n",
        "num_anchors = len(C.anchor_box_scales) * len(C.anchor_box_ratios)\n",
        "rpn = nn.rpn(shared_layers, num_anchors)\n",
        "\n",
        "classifier = nn.classifier(shared_layers, roi_input, C.num_rois, nb_classes=len(classes_count), trainable=True)\n",
        "\n",
        "model_rpn = Model(img_input, rpn[:2])\n",
        "model_classifier = Model([img_input, roi_input], classifier)\n",
        "\n",
        "# this is a model that holds both the RPN and the classifier, used to load/save weights for the models\n",
        "model_all = Model([img_input, roi_input], rpn[:2] + classifier)\n",
        "\n",
        "try:\n",
        "        print('loading weights from {}'.format(C.base_net_weights))\n",
        "\n",
        "        weights_path = get_file('base_weights.h5',C.base_net_weights)# downloading and adding weight paths\n",
        "        model_rpn.load_weights(weights_path)\n",
        "        model_classifier.load_weights(weights_path)\n",
        "        print('weights loaded.')\n",
        "except:\n",
        "        print('Could not load pretrained model weights. Weights can be found in the keras application folder \\\n",
        "                https://github.com/fchollet/keras/tree/master/keras/applications')\n",
        "\n",
        "optimizer = Adam(lr=1e-5)\n",
        "optimizer_classifier = Adam(lr=1e-5)\n",
        "model_rpn.compile(optimizer=optimizer, loss=[losses.rpn_loss_cls(num_anchors), losses.rpn_loss_regr(num_anchors)])\n",
        "model_classifier.compile(optimizer=optimizer_classifier, loss=[losses.class_loss_cls, losses.class_loss_regr(len(classes_count)-1)], metrics={'dense_class_{}'.format(len(classes_count)): 'accuracy'})\n",
        "model_all.compile(optimizer='sgd', loss='mae')\n",
        "\n",
        "epoch_length = 1000\n",
        "num_epochs = int(options.num_epochs)\n",
        "iter_num = 0\n",
        "\n",
        "losses = np.zeros((epoch_length, 5))\n",
        "rpn_accuracy_rpn_monitor = []\n",
        "rpn_accuracy_for_epoch = []\n",
        "start_time = time.time()\n",
        "\n",
        "best_loss = np.Inf\n",
        "\n",
        "class_mapping_inv = {v: k for k, v in class_mapping.items()}\n",
        "print('Starting training')\n",
        "\n",
        "vis = True\n",
        "\n",
        "for epoch_num in range(num_epochs):\n",
        "\n",
        "        progbar = generic_utils.Progbar(epoch_length)\n",
        "        print('Epoch {}/{}'.format(epoch_num + 1, num_epochs))\n",
        "\n",
        "        while True:\n",
        "                try:\n",
        "\n",
        "                        if len(rpn_accuracy_rpn_monitor) == epoch_length and C.verbose:\n",
        "                                mean_overlapping_bboxes = float(sum(rpn_accuracy_rpn_monitor))/len(rpn_accuracy_rpn_monitor)\n",
        "                                rpn_accuracy_rpn_monitor = []\n",
        "                                print('Average number of overlapping bounding boxes from RPN = {} for {} previous iterations'.format(mean_overlapping_bboxes, epoch_length))\n",
        "                                if mean_overlapping_bboxes == 0:\n",
        "                                        print('RPN is not producing bounding boxes that overlap the ground truth boxes. Check RPN settings or keep training.')\n",
        "\n",
        "                        X, Y, img_data = next(data_gen_train)\n",
        "\n",
        "                        loss_rpn = model_rpn.train_on_batch(X, Y)\n",
        "\n",
        "                        P_rpn = model_rpn.predict_on_batch(X)\n",
        "\n",
        "                        R = roi_helpers.rpn_to_roi(P_rpn[0], P_rpn[1], C, K.image_dim_ordering(), use_regr=True, overlap_thresh=0.7, max_boxes=300)\n",
        "                        # note: calc_iou converts from (x1,y1,x2,y2) to (x,y,w,h) format\n",
        "                        X2, Y1, Y2, IouS = roi_helpers.calc_iou(R, img_data, C, class_mapping)\n",
        "\n",
        "                        if X2 is None:\n",
        "                                rpn_accuracy_rpn_monitor.append(0)\n",
        "                                rpn_accuracy_for_epoch.append(0)\n",
        "                                continue\n",
        "\n",
        "                        neg_samples = np.where(Y1[0, :, -1] == 1)\n",
        "                        pos_samples = np.where(Y1[0, :, -1] == 0)\n",
        "\n",
        "                        if len(neg_samples) > 0:\n",
        "                                neg_samples = neg_samples[0]\n",
        "                        else:\n",
        "                                neg_samples = []\n",
        "\n",
        "                        if len(pos_samples) > 0:\n",
        "                                pos_samples = pos_samples[0]\n",
        "                        else:\n",
        "                                pos_samples = []\n",
        "                        \n",
        "                        rpn_accuracy_rpn_monitor.append(len(pos_samples))\n",
        "                        rpn_accuracy_for_epoch.append((len(pos_samples)))\n",
        "\n",
        "                        if C.num_rois > 1:\n",
        "                                if len(pos_samples) < C.num_rois//2:\n",
        "                                        selected_pos_samples = pos_samples.tolist()\n",
        "                                else:\n",
        "                                        selected_pos_samples = np.random.choice(pos_samples, C.num_rois//2, replace=False).tolist()\n",
        "                                try:\n",
        "                                        selected_neg_samples = np.random.choice(neg_samples, C.num_rois - len(selected_pos_samples), replace=False).tolist()\n",
        "                                except:\n",
        "                                        selected_neg_samples = np.random.choice(neg_samples, C.num_rois - len(selected_pos_samples), replace=True).tolist()\n",
        "\n",
        "                                sel_samples = selected_pos_samples + selected_neg_samples\n",
        "                        else:\n",
        "                                # in the extreme case where num_rois = 1, we pick a random pos or neg sample\n",
        "                                selected_pos_samples = pos_samples.tolist()\n",
        "                                selected_neg_samples = neg_samples.tolist()\n",
        "                                if np.random.randint(0, 2):\n",
        "                                        sel_samples = random.choice(neg_samples)\n",
        "                                else:\n",
        "                                        sel_samples = random.choice(pos_samples)\n",
        "\n",
        "                        loss_class = model_classifier.train_on_batch([X, X2[:, sel_samples, :]], [Y1[:, sel_samples, :], Y2[:, sel_samples, :]])\n",
        "\n",
        "                        losses[iter_num, 0] = loss_rpn[1]\n",
        "                        losses[iter_num, 1] = loss_rpn[2]\n",
        "\n",
        "                        losses[iter_num, 2] = loss_class[1]\n",
        "                        losses[iter_num, 3] = loss_class[2]\n",
        "                        losses[iter_num, 4] = loss_class[3]\n",
        "\n",
        "                        progbar.update(iter_num+1, [('rpn_cls', losses[iter_num, 0]), ('rpn_regr', losses[iter_num, 1]),\n",
        "                                                                          ('detector_cls', losses[iter_num, 2]), ('detector_regr', losses[iter_num, 3])])\n",
        "\n",
        "                        iter_num += 1\n",
        "                        \n",
        "                        if iter_num == epoch_length:\n",
        "                                loss_rpn_cls = np.mean(losses[:, 0])\n",
        "                                loss_rpn_regr = np.mean(losses[:, 1])\n",
        "                                loss_class_cls = np.mean(losses[:, 2])\n",
        "                                loss_class_regr = np.mean(losses[:, 3])\n",
        "                                class_acc = np.mean(losses[:, 4])\n",
        "\n",
        "                                mean_overlapping_bboxes = float(sum(rpn_accuracy_for_epoch)) / len(rpn_accuracy_for_epoch)\n",
        "                                rpn_accuracy_for_epoch = []\n",
        "\n",
        "                                if C.verbose:\n",
        "                                        print('Mean number of bounding boxes from RPN overlapping ground truth boxes: {}'.format(mean_overlapping_bboxes))\n",
        "                                        print('Classifier accuracy for bounding boxes from RPN: {}'.format(class_acc))\n",
        "                                        print('Loss RPN classifier: {}'.format(loss_rpn_cls))\n",
        "                                        print('Loss RPN regression: {}'.format(loss_rpn_regr))\n",
        "                                        print('Loss Detector classifier: {}'.format(loss_class_cls))\n",
        "                                        print('Loss Detector regression: {}'.format(loss_class_regr))\n",
        "                                        print('Elapsed time: {}'.format(time.time() - start_time))\n",
        "\n",
        "                                curr_loss = loss_rpn_cls + loss_rpn_regr + loss_class_cls + loss_class_regr\n",
        "                                iter_num = 0\n",
        "                                start_time = time.time()\n",
        "\n",
        "                                if curr_loss < best_loss:\n",
        "                                        if C.verbose:\n",
        "                                                print('Total loss decreased from {} to {}, saving weights'.format(best_loss,curr_loss))\n",
        "                                        best_loss = curr_loss\n",
        "                                        model_weights= 'model_frcnn_new.hdf5'\n",
        "                                        model_all.save_weights(model_weights)\n",
        "\n",
        "                                        with new_open(model_weights, mode='r') as infile:# to write hdf5 file to gs://input-your-bucket-name/train_on_gcloud/my_job_files/\n",
        "                                                with new_open(C.model_path + model_weights, mode='w+') as outfile:\n",
        "                                                        outfile.write(infile.read())\n",
        "\n",
        "                                break\n",
        "\n",
        "                except Exception as e:\n",
        "                        print('Exception: {}'.format(e))\n",
        "                        continue\n",
        "\n",
        "print('Training complete, exiting.')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting task.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0tuMyJ_wf9U",
        "colab_type": "text"
      },
      "source": [
        "* Exit out of **trainer/**, to the directory containing **setup.py** for initiating dependency packaging  before training on gcloud."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajSWRrHh0gea",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c0b9aee4-3a9e-414b-fded-005159a0b5b8"
      },
      "source": [
        "%cd ..\n",
        "!ls"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/training-frcnn-google-ml-engine/move_to_cloudshell\n",
            "annotations.txt  setup.py  trainer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqY_No38-Mu9",
        "colab_type": "text"
      },
      "source": [
        "* Define a JOB_NAME"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S74X6xVV_E_D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "JOB_NAME = 'test_job_gcloudColab'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPOQm0LEcXy4",
        "colab_type": "text"
      },
      "source": [
        "Run the following cell to package the **`trainer/`** directory:\n",
        "* It uploads the package to specified **gs://$BUCKET_NAME/JOB_NAME/**, and instruct AI Platform to run the **`trainer.task`** module from that package.\n",
        "\n",
        "* The **`--stream-logs`** flag lets you view training logs in the cell below (One can\n",
        "also view logs and other job details in the GCP Console, if you've enbaled **Stackdriver logging service**.)\n",
        "\n",
        "For staging to package and further training:\n",
        "* BUCKET_NAME = 'nifty-episode-231612-mlengine'\n",
        "* JOB_NAME = 'test_job_GoogleColab'\n",
        "* REGION= 'asia-east1'\n",
        "* package-path= trainer/\n",
        "* modele-name = trainer.task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbTYwWbR03C1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "5be1ffa7-0942-4dfe-cf0c-8944829af125"
      },
      "source": [
        "!ls trainer/"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "config.py\t\t    __init__.py     RoiPoolingConv.py\t   vgg.py\n",
            "data_augment.py\t\t    losses.py\t    simple_parser_pkl.py\n",
            "data_generators.py\t    resnet.py\t    simple_parser_text.py\n",
            "FixedBatchNormalization.py  roi_helpers.py  task.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dX1DXZZwW606",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7baed748-8f92-4cde-c02e-baa4bb9bc8ed"
      },
      "source": [
        "! gcloud ai-platform jobs submit training $JOB_NAME --package-path trainer/ --module-name trainer.task --region $REGION  --scale-tier=CUSTOM --master-machine-type=standard_gpu --staging-bucket gs://$BUCKET_NAME --stream-logs"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Job [test_job_gcloudColab] submitted successfully.\n",
            "INFO\t2019-07-26 05:53:21 +0000\tservice\t\tValidating job requirements...\n",
            "INFO\t2019-07-26 05:53:21 +0000\tservice\t\tJob creation request has been successfully validated.\n",
            "INFO\t2019-07-26 05:53:22 +0000\tservice\t\tWaiting for job to be provisioned.\n",
            "INFO\t2019-07-26 05:53:22 +0000\tservice\t\tJob test_job_gcloudColab is queued.\n",
            "INFO\t2019-07-26 05:53:24 +0000\tservice\t\tWaiting for training program to start.\n",
            "INFO\t2019-07-26 05:55:20 +0000\tmaster-replica-0\t\tRunning task with arguments: --cluster={\"master\": [\"127.0.0.1:2222\"]} --task={\"type\": \"master\", \"index\": 0} --job={  \"scale_tier\": \"CUSTOM\",  \"master_type\": \"standard_gpu\",  \"package_uris\": [\"gs://nifty-episode-231612-mlengine/test_job_gcloudColab/48b4b2e4a8eb7b77540ebec80d974c8d1f1a217b6c1e4f02297bb524c2d11c4b/frcnn_trainer-0.1.tar.gz\"],  \"python_module\": \"trainer.task\",  \"region\": \"asia-east1\",  \"run_on_raw_vm\": true}\n",
            "INFO\t2019-07-26 05:55:40 +0000\tmaster-replica-0\t\tRunning module trainer.task.\n",
            "INFO\t2019-07-26 05:55:40 +0000\tmaster-replica-0\t\tDownloading the package: gs://nifty-episode-231612-mlengine/test_job_gcloudColab/48b4b2e4a8eb7b77540ebec80d974c8d1f1a217b6c1e4f02297bb524c2d11c4b/frcnn_trainer-0.1.tar.gz\n",
            "INFO\t2019-07-26 05:55:40 +0000\tmaster-replica-0\t\tRunning command: gsutil -q cp gs://nifty-episode-231612-mlengine/test_job_gcloudColab/48b4b2e4a8eb7b77540ebec80d974c8d1f1a217b6c1e4f02297bb524c2d11c4b/frcnn_trainer-0.1.tar.gz frcnn_trainer-0.1.tar.gz\n",
            "INFO\t2019-07-26 05:55:41 +0000\tmaster-replica-0\t\tInstalling the package: gs://nifty-episode-231612-mlengine/test_job_gcloudColab/48b4b2e4a8eb7b77540ebec80d974c8d1f1a217b6c1e4f02297bb524c2d11c4b/frcnn_trainer-0.1.tar.gz\n",
            "INFO\t2019-07-26 05:55:41 +0000\tmaster-replica-0\t\tRunning command: pip install --user --upgrade --force-reinstall --no-deps frcnn_trainer-0.1.tar.gz\n",
            "ERROR\t2019-07-26 05:55:43 +0000\tmaster-replica-0\t\tDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7.\n",
            "INFO\t2019-07-26 05:55:43 +0000\tmaster-replica-0\t\tProcessing ./frcnn_trainer-0.1.tar.gz\n",
            "INFO\t2019-07-26 05:55:43 +0000\tmaster-replica-0\t\tBuilding wheels for collected packages: frcnn-trainer\n",
            "INFO\t2019-07-26 05:55:43 +0000\tmaster-replica-0\t\t  Building wheel for frcnn-trainer (setup.py): started\n",
            "INFO\t2019-07-26 05:55:44 +0000\tmaster-replica-0\t\tcreating '/tmp/pip-wheel-TwnJnL/frcnn_trainer-0.1-cp27-none-any.whl' and adding '.' to it\n",
            "INFO\t2019-07-26 05:55:44 +0000\tmaster-replica-0\t\tadding 'trainer/data_generators.py'\n",
            "INFO\t2019-07-26 05:55:44 +0000\tmaster-replica-0\t\tadding 'trainer/vgg.py'\n",
            "INFO\t2019-07-26 05:55:44 +0000\tmaster-replica-0\t\tadding 'trainer/losses.py'\n",
            "INFO\t2019-07-26 05:55:44 +0000\tmaster-replica-0\t\tadding 'trainer/simple_parser_pkl.py'\n",
            "INFO\t2019-07-26 05:55:44 +0000\tmaster-replica-0\t\tadding 'trainer/simple_parser_text.py'\n",
            "INFO\t2019-07-26 05:55:44 +0000\tmaster-replica-0\t\tadding 'trainer/config.py'\n",
            "INFO\t2019-07-26 05:55:44 +0000\tmaster-replica-0\t\tadding 'trainer/__init__.py'\n",
            "INFO\t2019-07-26 05:55:44 +0000\tmaster-replica-0\t\tadding 'trainer/RoiPoolingConv.py'\n",
            "INFO\t2019-07-26 05:55:44 +0000\tmaster-replica-0\t\tadding 'trainer/resnet.py'\n",
            "INFO\t2019-07-26 05:55:44 +0000\tmaster-replica-0\t\tadding 'trainer/roi_helpers.py'\n",
            "INFO\t2019-07-26 05:55:44 +0000\tmaster-replica-0\t\tadding 'trainer/task.py'\n",
            "INFO\t2019-07-26 05:55:44 +0000\tmaster-replica-0\t\tadding 'trainer/data_augment.py'\n",
            "INFO\t2019-07-26 05:55:44 +0000\tmaster-replica-0\t\tadding 'trainer/FixedBatchNormalization.py'\n",
            "INFO\t2019-07-26 05:55:44 +0000\tmaster-replica-0\t\tadding 'frcnn_trainer-0.1.dist-info/DESCRIPTION.rst'\n",
            "INFO\t2019-07-26 05:55:44 +0000\tmaster-replica-0\t\tadding 'frcnn_trainer-0.1.dist-info/metadata.json'\n",
            "INFO\t2019-07-26 05:55:44 +0000\tmaster-replica-0\t\tadding 'frcnn_trainer-0.1.dist-info/top_level.txt'\n",
            "INFO\t2019-07-26 05:55:44 +0000\tmaster-replica-0\t\tadding 'frcnn_trainer-0.1.dist-info/WHEEL'\n",
            "INFO\t2019-07-26 05:55:44 +0000\tmaster-replica-0\t\tadding 'frcnn_trainer-0.1.dist-info/METADATA'\n",
            "INFO\t2019-07-26 05:55:44 +0000\tmaster-replica-0\t\tadding 'frcnn_trainer-0.1.dist-info/RECORD'\n",
            "INFO\t2019-07-26 05:55:44 +0000\tmaster-replica-0\t\t  Building wheel for frcnn-trainer (setup.py): finished with status 'done'\n",
            "INFO\t2019-07-26 05:55:44 +0000\tmaster-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/5f/dc/f1/44f4d4c2d7e4540b60cecc409bf0e8c1347e3d2de9032c76c7\n",
            "INFO\t2019-07-26 05:55:44 +0000\tmaster-replica-0\t\tSuccessfully built frcnn-trainer\n",
            "INFO\t2019-07-26 05:55:44 +0000\tmaster-replica-0\t\tInstalling collected packages: frcnn-trainer\n",
            "INFO\t2019-07-26 05:55:44 +0000\tmaster-replica-0\t\tSuccessfully installed frcnn-trainer-0.1\n",
            "INFO\t2019-07-26 05:55:44 +0000\tmaster-replica-0\t\tRunning command: pip install --user frcnn_trainer-0.1.tar.gz\n",
            "ERROR\t2019-07-26 05:55:44 +0000\tmaster-replica-0\t\tDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7.\n",
            "INFO\t2019-07-26 05:55:44 +0000\tmaster-replica-0\t\tProcessing ./frcnn_trainer-0.1.tar.gz\n",
            "INFO\t2019-07-26 05:55:45 +0000\tmaster-replica-0\t\tCollecting pillow (from frcnn-trainer==0.1)\n",
            "INFO\t2019-07-26 05:55:45 +0000\tmaster-replica-0\t\t  Downloading https://files.pythonhosted.org/packages/cc/a4/79b5f36d1e1a2b426073bd62217d1530fcd939950c2936651e6b39127a9b/Pillow-6.1.0-cp27-cp27mu-manylinux1_x86_64.whl (2.1MB)\n",
            "INFO\t2019-07-26 05:55:46 +0000\tmaster-replica-0\t\tCollecting keras (from frcnn-trainer==0.1)\n",
            "INFO\t2019-07-26 05:55:46 +0000\tmaster-replica-0\t\t  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\n",
            "INFO\t2019-07-26 05:55:46 +0000\tmaster-replica-0\t\tCollecting h5py (from frcnn-trainer==0.1)\n",
            "INFO\t2019-07-26 05:55:46 +0000\tmaster-replica-0\t\t  Downloading https://files.pythonhosted.org/packages/53/08/27e4e9a369321862ffdce80ff1770553e9daec65d98befb2e14e7478b698/h5py-2.9.0-cp27-cp27mu-manylinux1_x86_64.whl (2.8MB)\n",
            "INFO\t2019-07-26 05:55:46 +0000\tmaster-replica-0\t\tRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python2.7/dist-packages (from keras->frcnn-trainer==0.1) (1.10.0)\n",
            "INFO\t2019-07-26 05:55:46 +0000\tmaster-replica-0\t\tCollecting keras-applications>=1.0.6 (from keras->frcnn-trainer==0.1)\n",
            "INFO\t2019-07-26 05:55:46 +0000\tmaster-replica-0\t\t  Downloading https://files.pythonhosted.org/packages/21/56/4bcec5a8d9503a87e58e814c4e32ac2b32c37c685672c30bc8c54c6e478a/Keras_Applications-1.0.8.tar.gz (289kB)\n",
            "INFO\t2019-07-26 05:55:46 +0000\tmaster-replica-0\t\tCollecting keras-preprocessing>=1.0.5 (from keras->frcnn-trainer==0.1)\n",
            "INFO\t2019-07-26 05:55:46 +0000\tmaster-replica-0\t\t  Downloading https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41kB)\n",
            "INFO\t2019-07-26 05:55:46 +0000\tmaster-replica-0\t\tRequirement already satisfied: pyyaml in /usr/local/lib/python2.7/dist-packages (from keras->frcnn-trainer==0.1) (3.11)\n",
            "INFO\t2019-07-26 05:55:46 +0000\tmaster-replica-0\t\tRequirement already satisfied: scipy>=0.14 in /usr/local/lib/python2.7/dist-packages (from keras->frcnn-trainer==0.1) (0.17.0)\n",
            "INFO\t2019-07-26 05:55:46 +0000\tmaster-replica-0\t\tRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python2.7/dist-packages (from keras->frcnn-trainer==0.1) (1.12.1)\n",
            "INFO\t2019-07-26 05:55:46 +0000\tmaster-replica-0\t\tBuilding wheels for collected packages: frcnn-trainer, keras-applications\n",
            "INFO\t2019-07-26 05:55:46 +0000\tmaster-replica-0\t\t  Building wheel for frcnn-trainer (setup.py): started\n",
            "INFO\t2019-07-26 05:55:47 +0000\tmaster-replica-0\t\tcreating '/tmp/pip-wheel-xScAHk/frcnn_trainer-0.1-cp27-none-any.whl' and adding '.' to it\n",
            "INFO\t2019-07-26 05:55:47 +0000\tmaster-replica-0\t\tadding 'trainer/data_generators.py'\n",
            "INFO\t2019-07-26 05:55:47 +0000\tmaster-replica-0\t\tadding 'trainer/vgg.py'\n",
            "INFO\t2019-07-26 05:55:47 +0000\tmaster-replica-0\t\tadding 'trainer/losses.py'\n",
            "INFO\t2019-07-26 05:55:47 +0000\tmaster-replica-0\t\tadding 'trainer/simple_parser_pkl.py'\n",
            "INFO\t2019-07-26 05:55:47 +0000\tmaster-replica-0\t\tadding 'trainer/simple_parser_text.py'\n",
            "INFO\t2019-07-26 05:55:47 +0000\tmaster-replica-0\t\tadding 'trainer/config.py'\n",
            "INFO\t2019-07-26 05:55:47 +0000\tmaster-replica-0\t\tadding 'trainer/__init__.py'\n",
            "INFO\t2019-07-26 05:55:47 +0000\tmaster-replica-0\t\tadding 'trainer/RoiPoolingConv.py'\n",
            "INFO\t2019-07-26 05:55:47 +0000\tmaster-replica-0\t\tadding 'trainer/resnet.py'\n",
            "INFO\t2019-07-26 05:55:47 +0000\tmaster-replica-0\t\tadding 'trainer/roi_helpers.py'\n",
            "INFO\t2019-07-26 05:55:47 +0000\tmaster-replica-0\t\tadding 'trainer/task.py'\n",
            "INFO\t2019-07-26 05:55:47 +0000\tmaster-replica-0\t\tadding 'trainer/data_augment.py'\n",
            "INFO\t2019-07-26 05:55:47 +0000\tmaster-replica-0\t\tadding 'trainer/FixedBatchNormalization.py'\n",
            "INFO\t2019-07-26 05:55:47 +0000\tmaster-replica-0\t\tadding 'frcnn_trainer-0.1.dist-info/DESCRIPTION.rst'\n",
            "INFO\t2019-07-26 05:55:47 +0000\tmaster-replica-0\t\tadding 'frcnn_trainer-0.1.dist-info/metadata.json'\n",
            "INFO\t2019-07-26 05:55:47 +0000\tmaster-replica-0\t\tadding 'frcnn_trainer-0.1.dist-info/top_level.txt'\n",
            "INFO\t2019-07-26 05:55:47 +0000\tmaster-replica-0\t\tadding 'frcnn_trainer-0.1.dist-info/WHEEL'\n",
            "INFO\t2019-07-26 05:55:47 +0000\tmaster-replica-0\t\tadding 'frcnn_trainer-0.1.dist-info/METADATA'\n",
            "INFO\t2019-07-26 05:55:47 +0000\tmaster-replica-0\t\tadding 'frcnn_trainer-0.1.dist-info/RECORD'\n",
            "INFO\t2019-07-26 05:55:47 +0000\tmaster-replica-0\t\t  Building wheel for frcnn-trainer (setup.py): finished with status 'done'\n",
            "INFO\t2019-07-26 05:55:47 +0000\tmaster-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/5f/dc/f1/44f4d4c2d7e4540b60cecc409bf0e8c1347e3d2de9032c76c7\n",
            "INFO\t2019-07-26 05:55:47 +0000\tmaster-replica-0\t\t  Building wheel for keras-applications (setup.py): started\n",
            "INFO\t2019-07-26 05:55:47 +0000\tmaster-replica-0\t\tcreating '/tmp/pip-wheel-t2hJj1/Keras_Applications-1.0.8-cp27-none-any.whl' and adding '.' to it\n",
            "INFO\t2019-07-26 05:55:47 +0000\tmaster-replica-0\t\tadding 'keras_applications/resnext.py'\n",
            "INFO\t2019-07-26 05:55:47 +0000\tmaster-replica-0\t\tadding 'keras_applications/densenet.py'\n",
            "INFO\t2019-07-26 05:55:47 +0000\tmaster-replica-0\t\tadding 'keras_applications/imagenet_utils.py'\n",
            "INFO\t2019-07-26 05:55:47 +0000\tmaster-replica-0\t\tadding 'keras_applications/resnet_common.py'\n",
            "INFO\t2019-07-26 05:55:47 +0000\tmaster-replica-0\t\tadding 'keras_applications/resnet50.py'\n",
            "INFO\t2019-07-26 05:55:47 +0000\tmaster-replica-0\t\tadding 'keras_applications/mobilenet_v2.py'\n",
            "INFO\t2019-07-26 05:55:47 +0000\tmaster-replica-0\t\tadding 'keras_applications/xception.py'\n",
            "INFO\t2019-07-26 05:55:47 +0000\tmaster-replica-0\t\tadding 'keras_applications/__init__.py'\n",
            "INFO\t2019-07-26 05:55:47 +0000\tmaster-replica-0\t\tadding 'keras_applications/vgg16.py'\n",
            "INFO\t2019-07-26 05:55:47 +0000\tmaster-replica-0\t\tadding 'keras_applications/vgg19.py'\n",
            "INFO\t2019-07-26 05:55:47 +0000\tmaster-replica-0\t\tadding 'keras_applications/mobilenet.py'\n",
            "INFO\t2019-07-26 05:55:47 +0000\tmaster-replica-0\t\tadding 'keras_applications/resnet.py'\n",
            "INFO\t2019-07-26 05:55:47 +0000\tmaster-replica-0\t\tadding 'keras_applications/inception_v3.py'\n",
            "INFO\t2019-07-26 05:55:47 +0000\tmaster-replica-0\t\tadding 'keras_applications/inception_resnet_v2.py'\n",
            "INFO\t2019-07-26 05:55:47 +0000\tmaster-replica-0\t\tadding 'keras_applications/nasnet.py'\n",
            "INFO\t2019-07-26 05:55:47 +0000\tmaster-replica-0\t\tadding 'keras_applications/resnet_v2.py'\n",
            "INFO\t2019-07-26 05:55:47 +0000\tmaster-replica-0\t\tadding 'Keras_Applications-1.0.8.dist-info/DESCRIPTION.rst'\n",
            "INFO\t2019-07-26 05:55:47 +0000\tmaster-replica-0\t\tadding 'Keras_Applications-1.0.8.dist-info/metadata.json'\n",
            "INFO\t2019-07-26 05:55:47 +0000\tmaster-replica-0\t\tadding 'Keras_Applications-1.0.8.dist-info/top_level.txt'\n",
            "INFO\t2019-07-26 05:55:47 +0000\tmaster-replica-0\t\tadding 'Keras_Applications-1.0.8.dist-info/WHEEL'\n",
            "INFO\t2019-07-26 05:55:47 +0000\tmaster-replica-0\t\tadding 'Keras_Applications-1.0.8.dist-info/METADATA'\n",
            "INFO\t2019-07-26 05:55:47 +0000\tmaster-replica-0\t\tadding 'Keras_Applications-1.0.8.dist-info/RECORD'\n",
            "INFO\t2019-07-26 05:55:47 +0000\tmaster-replica-0\t\t  Building wheel for keras-applications (setup.py): finished with status 'done'\n",
            "INFO\t2019-07-26 05:55:47 +0000\tmaster-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/dd/f2/5d/2689b5547f32c4e258c3b7ccbe7f1d0f2afbb84fb01e830792\n",
            "INFO\t2019-07-26 05:55:47 +0000\tmaster-replica-0\t\tSuccessfully built frcnn-trainer keras-applications\n",
            "INFO\t2019-07-26 05:55:48 +0000\tmaster-replica-0\t\tInstalling collected packages: pillow, h5py, keras-applications, keras-preprocessing, keras, frcnn-trainer\n",
            "INFO\t2019-07-26 05:55:48 +0000\tmaster-replica-0\t\t  Found existing installation: frcnn-trainer 0.1\n",
            "INFO\t2019-07-26 05:55:48 +0000\tmaster-replica-0\t\t    Uninstalling frcnn-trainer-0.1:\n",
            "INFO\t2019-07-26 05:55:48 +0000\tmaster-replica-0\t\t      Successfully uninstalled frcnn-trainer-0.1\n",
            "INFO\t2019-07-26 05:55:48 +0000\tmaster-replica-0\t\tSuccessfully installed frcnn-trainer-0.1 h5py-2.9.0 keras-2.2.4 keras-applications-1.0.8 keras-preprocessing-1.1.0 pillow-6.1.0\n",
            "INFO\t2019-07-26 05:55:48 +0000\tmaster-replica-0\t\tRunning command: python -m trainer.task\n",
            "INFO\t2019-07-26 05:55:48 +0000\tmaster-replica-0\t\tsuccessfully opened CUDA library libcublas.so.8.0 locally\n",
            "INFO\t2019-07-26 05:55:49 +0000\tmaster-replica-0\t\tsuccessfully opened CUDA library libcudnn.so.5 locally\n",
            "INFO\t2019-07-26 05:55:49 +0000\tmaster-replica-0\t\tsuccessfully opened CUDA library libcufft.so.8.0 locally\n",
            "INFO\t2019-07-26 05:55:49 +0000\tmaster-replica-0\t\tsuccessfully opened CUDA library libcuda.so.1 locally\n",
            "INFO\t2019-07-26 05:55:49 +0000\tmaster-replica-0\t\tsuccessfully opened CUDA library libcurand.so.8.0 locally\n",
            "ERROR\t2019-07-26 05:55:49 +0000\tmaster-replica-0\t\tUsing TensorFlow backend.\n",
            "ERROR\t2019-07-26 05:55:54 +0000\tmaster-replica-0\t\tlibdc1394 error: Failed to initialize libdc1394\n",
            "ERROR\t2019-07-26 05:55:54 +0000\tmaster-replica-0\t\tTraceback (most recent call last):\n",
            "ERROR\t2019-07-26 05:55:54 +0000\tmaster-replica-0\t\t  File \"/usr/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n",
            "ERROR\t2019-07-26 05:55:54 +0000\tmaster-replica-0\t\t    \"__main__\", fname, loader, pkg_name)\n",
            "ERROR\t2019-07-26 05:55:54 +0000\tmaster-replica-0\t\t  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n",
            "ERROR\t2019-07-26 05:55:54 +0000\tmaster-replica-0\t\t    exec code in run_globals\n",
            "ERROR\t2019-07-26 05:55:54 +0000\tmaster-replica-0\t\t  File \"/root/.local/lib/python2.7/site-packages/trainer/task.py\", line 71, in <module>\n",
            "ERROR\t2019-07-26 05:55:54 +0000\tmaster-replica-0\t\t    from keras_frcnn import resnet as nn\n",
            "ERROR\t2019-07-26 05:55:54 +0000\tmaster-replica-0\t\tImportError: No module named keras_frcnn\n",
            "ERROR\t2019-07-26 05:55:55 +0000\tmaster-replica-0\t\tCommand '['python', '-m', u'trainer.task']' returned non-zero exit status 1\n",
            "INFO\t2019-07-26 05:55:55 +0000\tmaster-replica-0\t\tModule completed; cleaning up.\n",
            "INFO\t2019-07-26 05:55:55 +0000\tmaster-replica-0\t\tClean up finished.\n",
            "ERROR\t2019-07-26 05:56:19 +0000\tservice\t\tThe replica master 0 exited with a non-zero status of 1. \n",
            "ERROR\t2019-07-26 05:56:19 +0000\tservice\t\tTraceback (most recent call last):\n",
            "ERROR\t2019-07-26 05:56:19 +0000\tservice\t\t  File \"/usr/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n",
            "ERROR\t2019-07-26 05:56:19 +0000\tservice\t\t    \"__main__\", fname, loader, pkg_name)\n",
            "ERROR\t2019-07-26 05:56:19 +0000\tservice\t\t  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n",
            "ERROR\t2019-07-26 05:56:19 +0000\tservice\t\t    exec code in run_globals\n",
            "ERROR\t2019-07-26 05:56:19 +0000\tservice\t\t  File \"/root/.local/lib/python2.7/site-packages/trainer/task.py\", line 71, in <module>\n",
            "ERROR\t2019-07-26 05:56:19 +0000\tservice\t\t    from keras_frcnn import resnet as nn\n",
            "ERROR\t2019-07-26 05:56:19 +0000\tservice\t\tImportError: No module named keras_frcnn\n",
            "ERROR\t2019-07-26 05:56:19 +0000\tservice\t\t\n",
            "ERROR\t2019-07-26 05:56:19 +0000\tservice\t\tTo find out more about why your job exited please check the logs: https://console.cloud.google.com/logs/viewer?project=859885312311&resource=ml_job%2Fjob_id%2Ftest_job_gcloudColab&advancedFilter=resource.type%3D%22ml_job%22%0Aresource.labels.job_id%3D%22test_job_gcloudColab%22\n",
            "endTime: '2019-07-26T05:58:52'\n",
            "jobId: test_job_gcloudColab\n",
            "startTime: '2019-07-26T05:55:19'\n",
            "state: FAILED\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KM3Q71nHCi5W",
        "colab_type": "text"
      },
      "source": [
        "## 2. Online predictions in AI Platform\n",
        "\n",
        "### Create model and version resources in AI Platform\n",
        "\n",
        "To serve online predictions using the model you trained and exported in Part 1,\n",
        "create a *model* resource in AI Platform and a *version* resource\n",
        "within it. The version resource is what actually uses your trained model to\n",
        "serve predictions. This structure lets you adjust and retrain your model many times and\n",
        "organize all the versions together in AI Platform. Learn more about [models\n",
        "and\n",
        "versions](https://cloud.google.com/ml-engine/docs/tensorflow/projects-models-versions-jobs).\n",
        "\n",
        "\n",
        "* First, Define a name and create the model resource;\n",
        "Also Enable Online prediction logging, to stream logs that contain the **stderr and stdout streams** from your prediction nodes, and can be useful for debugging during version creation and inferencing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13j3uYJFDYoj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_NAME = \"food_predictor\"\n",
        "\n",
        "! gcloud beta ai-platform models create $MODEL_NAME \\\n",
        "  --regions $REGION --enable-console-logging"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuv_4mMZDZIU",
        "colab_type": "text"
      },
      "source": [
        "Next, create the model version. The training job is exported to a timestamped directory in your Cloud Storage bucket. AI Platform uses this directory to create a model version.\n",
        "Learn more about [SavedModel and\n",
        "AI Platform](https://cloud.google.com/ml-engine/docs/tensorflow/deploying-models).\n",
        "\n",
        "You may be able to find the path to this directory in your training job's logs.\n",
        "Look for a line like:\n",
        "\n",
        "```\n",
        "Model exported to:  gs://<your-bucket-name>/keras-job-dir/keras_export/1545439782\n",
        "```\n",
        "\n",
        "Execute the following command to identify your SavedModel directory and use it to create a model version resource:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_28dC8MF3NU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "python setup.py sdist --formats=gztar\n",
        "\n",
        "gsutil cp dist/test_code_new_model_beta5-0.1.tar.gz gs://nifty-episode-231612-mlengine/cloud_test_package_2/cloud_test_package_v5\n",
        "\n",
        "MODEL_NAME=\"FoodPredictor_060619\"\n",
        "VERSION_NAME='v5_a'\n",
        "REGION=asia-east1\n",
        "\n",
        "\n",
        "gcloud beta ai-platform versions create $VERSION_NAME --model $MODEL_NAME\n",
        "--python-version 3.5 --runtime-version 1.5 --machine-type mls1-c4-m2\n",
        "--origin gs://nifty-episode-231612-mlengine/cloud_test_package_2/cloud_test_package_v5\n",
        "--package-uris gs://nifty-episode-231612-mlengine/cloud_test_package_2/cloud_test_package_v5/test_code_new_model_beta5-0.1.tar.gz\n",
        "--prediction-class predictor.MyPredictor\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9a-Myf8W63U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "898b368c-e17c-44dc-eb4f-6ea500acad93"
      },
      "source": [
        ""
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[m\u001b[1mNAME\u001b[m\n",
            "    gcloud - manage Google Cloud Platform resources and developer workflow\n",
            "\n",
            "\u001b[m\u001b[1mSYNOPSIS\u001b[m\n",
            "    \u001b[1mgcloud\u001b[m \u001b[4mGROUP\u001b[m | \u001b[4mCOMMAND\u001b[m [\u001b[1m--account\u001b[m=\u001b[4mACCOUNT\u001b[m]\n",
            "        [\u001b[1m--billing-project\u001b[m=\u001b[4mBILLING_PROJECT\u001b[m] [\u001b[1m--configuration\u001b[m=\u001b[4mCONFIGURATION\u001b[m]\n",
            "        [\u001b[1m--flags-file\u001b[m=\u001b[4mYAML_FILE\u001b[m] [\u001b[1m--flatten\u001b[m=[\u001b[4mKEY\u001b[m,...]] [\u001b[1m--format\u001b[m=\u001b[4mFORMAT\u001b[m]\n",
            "        [\u001b[1m--help\u001b[m] [\u001b[1m--project\u001b[m=\u001b[4mPROJECT_ID\u001b[m] [\u001b[1m--quiet\u001b[m, \u001b[1m-q\u001b[m]\n",
            "        [\u001b[1m--verbosity\u001b[m=\u001b[4mVERBOSITY\u001b[m; default=\"warning\"] [\u001b[1m--version\u001b[m, \u001b[1m-v\u001b[m] [\u001b[1m-h\u001b[m]\n",
            "        [\u001b[1m--impersonate-service-account\u001b[m=\u001b[4mSERVICE_ACCOUNT_EMAIL\u001b[m] [\u001b[1m--log-http\u001b[m]\n",
            "        [\u001b[1m--trace-token\u001b[m=\u001b[4mTRACE_TOKEN\u001b[m] [\u001b[1m--no-user-output-enabled\u001b[m]\n",
            "\n",
            "\u001b[m\u001b[1mDESCRIPTION\u001b[m\n",
            "    The \u001b[1mgcloud\u001b[m CLI manages authentication, local configuration, developer\n",
            "    workflow, and interactions with the Google Cloud Platform APIs.\n",
            "\n",
            "\u001b[m\u001b[1mGLOBAL FLAGS\u001b[m\n",
            "     \u001b[1m--account\u001b[m=\u001b[4mACCOUNT\u001b[m\n",
            "        Google Cloud Platform user account to use for invocation. Overrides the\n",
            "        default \u001b[1mcore/account\u001b[m property value for this command invocation.\n",
            "\n",
            "     \u001b[1m--billing-project\u001b[m=\u001b[4mBILLING_PROJECT\u001b[m\n",
            "        The Google Cloud Platform project that will be charged quota for\n",
            "        operations performed in gcloud. If you need to operate on one project,\n",
            "        but need quota against a different project, you can use this flag to\n",
            "        specify the billing project. If both \u001b[1mbilling/quota_project\u001b[m and\n",
            "        \u001b[1m--billing-project\u001b[m are specified, \u001b[1m--billing-project\u001b[m takes precedence.\n",
            "        Run \u001b[1m$ gcloud config set --help\u001b[m to see more information about\n",
            "        \u001b[1mbilling/quota_project\u001b[m.\n",
            "\n",
            "     \u001b[1m--configuration\u001b[m=\u001b[4mCONFIGURATION\u001b[m\n",
            "        The configuration to use for this command invocation. For more\n",
            "        information on how to use configurations, run: \u001b[1mgcloud topic\n",
            "        configurations\u001b[m. You can also use the [CLOUDSDK_ACTIVE_CONFIG_NAME]\n",
            "        environment variable to set the equivalent of this flag for a terminal\n",
            "        session.\n",
            "\n",
            "     \u001b[1m--flags-file\u001b[m=\u001b[4mYAML_FILE\u001b[m\n",
            "        A YAML or JSON file that specifies a \u001b[1m--flag\u001b[m:\u001b[1mvalue\u001b[m dictionary. Useful\n",
            "        for specifying complex flag values with special characters that work\n",
            "        with any command interpreter. Additionally, each \u001b[1m--flags-file\u001b[m arg is\n",
            "        replaced by its constituent flags. See $ gcloud topic flags-file for\n",
            "        more information.\n",
            "\n",
            "     \u001b[1m--flatten\u001b[m=[\u001b[4mKEY\u001b[m,...]\n",
            "        Flatten \u001b[4mname\u001b[m[] output resource slices in \u001b[4mKEY\u001b[m into separate records for\n",
            "        each item in each slice. Multiple keys and slices may be specified.\n",
            "        This also flattens keys for \u001b[1m--format\u001b[m and \u001b[1m--filter\u001b[m. For example,\n",
            "        \u001b[1m--flatten=abc.def\u001b[m flattens \u001b[1mabc.def[].ghi\u001b[m references to \u001b[1mabc.def.ghi\u001b[m. A\n",
            "        resource record containing \u001b[1mabc.def[]\u001b[m with N elements will expand to N\n",
            "        records in the flattened output. This flag interacts with other flags\n",
            "        that are applied in this order: \u001b[1m--flatten\u001b[m, \u001b[1m--sort-by\u001b[m, \u001b[1m--filter\u001b[m,\n",
            "        \u001b[1m--limit\u001b[m.\n",
            "\n",
            "     \u001b[1m--format\u001b[m=\u001b[4mFORMAT\u001b[m\n",
            "        Set the format for printing command output resources. The default is a\n",
            "        command-specific human-friendly output format. The supported formats\n",
            "        are: \u001b[1mconfig\u001b[m, \u001b[1mcsv\u001b[m, \u001b[1mdefault\u001b[m, \u001b[1mdiff\u001b[m, \u001b[1mdisable\u001b[m, \u001b[1mflattened\u001b[m, \u001b[1mget\u001b[m, \u001b[1mjson\u001b[m, \u001b[1mlist\u001b[m,\n",
            "        \u001b[1mmulti\u001b[m, \u001b[1mnone\u001b[m, \u001b[1mobject\u001b[m, \u001b[1mtable\u001b[m, \u001b[1mtext\u001b[m, \u001b[1mvalue\u001b[m, \u001b[1myaml\u001b[m. For more details run $\n",
            "        gcloud topic formats.\n",
            "\n",
            "     \u001b[1m--help\u001b[m\n",
            "        Display detailed help.\n",
            "\n",
            "     \u001b[1m--project\u001b[m=\u001b[4mPROJECT_ID\u001b[m\n",
            "        The Google Cloud Platform project name to use for this invocation. If\n",
            "        omitted, then the current project is assumed; the current project can\n",
            "        be listed using \u001b[1mgcloud config list --format='text(core.project)'\u001b[m and\n",
            "        can be set using \u001b[1mgcloud config set project PROJECTID\u001b[m. Overrides the\n",
            "        default \u001b[1mcore/project\u001b[m property value for this command invocation.\n",
            "\n",
            "     \u001b[1m--quiet\u001b[m, \u001b[1m-q\u001b[m\n",
            "        Disable all interactive prompts when running gcloud commands. If input\n",
            "        is required, defaults will be used, or an error will be raised.\n",
            "        Overrides the default core/disable_prompts property value for this\n",
            "        command invocation. Must be used at the beginning of commands. This is\n",
            "        equivalent to setting the environment variable\n",
            "        \u001b[1mCLOUDSDK_CORE_DISABLE_PROMPTS\u001b[m to 1.\n",
            "\n",
            "     \u001b[1m--verbosity\u001b[m=\u001b[4mVERBOSITY\u001b[m; default=\"warning\"\n",
            "        Override the default verbosity for this command. Overrides the default\n",
            "        \u001b[1mcore/verbosity\u001b[m property value for this command invocation. \u001b[4mVERBOSITY\u001b[m\n",
            "        must be one of: \u001b[1mdebug\u001b[m, \u001b[1minfo\u001b[m, \u001b[1mwarning\u001b[m, \u001b[1merror\u001b[m, \u001b[1mcritical\u001b[m, \u001b[1mnone\u001b[m.\n",
            "\n",
            "     \u001b[1m--version\u001b[m, \u001b[1m-v\u001b[m\n",
            "        Print version information and exit. This flag is only available at the\n",
            "        global level.\n",
            "\n",
            "     \u001b[1m-h\u001b[m\n",
            "        Print a summary help and exit.\n",
            "\n",
            "\u001b[m\u001b[1mOTHER FLAGS\u001b[m\n",
            "     \u001b[1m--impersonate-service-account\u001b[m=\u001b[4mSERVICE_ACCOUNT_EMAIL\u001b[m\n",
            "        For this gcloud invocation, all API requests will be made as the given\n",
            "        service account instead of the currently selected account. This is done\n",
            "        without needing to create, download, and activate a key for the\n",
            "        account. In order to perform operations as the service account, your\n",
            "        currently selected account must have an IAM role that includes the\n",
            "        iam.serviceAccounts.getAccessToken permission for the service account.\n",
            "        The roles/iam.serviceAccountTokenCreator role has this permission or\n",
            "        you may create a custom role. Overrides the default\n",
            "        \u001b[1mauth/impersonate_service_account\u001b[m property value for this command\n",
            "        invocation.\n",
            "\n",
            "     \u001b[1m--log-http\u001b[m\n",
            "        Log all HTTP server requests and responses to stderr. Overrides the\n",
            "        default \u001b[1mcore/log_http\u001b[m property value for this command invocation.\n",
            "\n",
            "     \u001b[1m--trace-token\u001b[m=\u001b[4mTRACE_TOKEN\u001b[m\n",
            "        Token used to route traces of service requests for investigation of\n",
            "        issues. Overrides the default \u001b[1mcore/trace_token\u001b[m property value for this\n",
            "        command invocation.\n",
            "\n",
            "     \u001b[1m--user-output-enabled\u001b[m\n",
            "        Print user intended output to the console. Overrides the default\n",
            "        \u001b[1mcore/user_output_enabled\u001b[m property value for this command invocation.\n",
            "        Use \u001b[1m--no-user-output-enabled\u001b[m to disable.\n",
            "\n",
            "\u001b[m\u001b[1mGROUPS\u001b[m\n",
            "    \u001b[1m\u001b[1;4mGROUP\u001b[1m\u001b[m is one of the following:\n",
            "\n",
            "     \u001b[1maccess-context-manager\u001b[m\n",
            "        Manage Access Context Manager resources.\n",
            "\n",
            "     \u001b[1mai-platform\u001b[m\n",
            "        Manage AI Platform jobs and models.\n",
            "\n",
            "     \u001b[1malpha\u001b[m\n",
            "        \u001b[1m(ALPHA)\u001b[m Alpha versions of gcloud commands.\n",
            "\n",
            "     \u001b[1mapp\u001b[m\n",
            "        Manage your App Engine deployments.\n",
            "\n",
            "     \u001b[1masset\u001b[m\n",
            "        Manage the Cloud Asset Inventory.\n",
            "\n",
            "     \u001b[1mauth\u001b[m\n",
            "        Manage oauth2 credentials for the Google Cloud SDK.\n",
            "\n",
            "     \u001b[1mbeta\u001b[m\n",
            "        \u001b[1m(BETA)\u001b[m Beta versions of gcloud commands.\n",
            "\n",
            "     \u001b[1mbigtable\u001b[m\n",
            "        Manage your Cloud Bigtable storage.\n",
            "\n",
            "     \u001b[1mbuilds\u001b[m\n",
            "        Create and manage builds for Google Cloud Build.\n",
            "\n",
            "     \u001b[1mcomponents\u001b[m\n",
            "        List, install, update, or remove Google Cloud SDK components.\n",
            "\n",
            "     \u001b[1mcomposer\u001b[m\n",
            "        Create and manage Cloud Composer Environments.\n",
            "\n",
            "     \u001b[1mcompute\u001b[m\n",
            "        Create and manipulate Google Compute Engine resources.\n",
            "\n",
            "     \u001b[1mconfig\u001b[m\n",
            "        View and edit Cloud SDK properties.\n",
            "\n",
            "     \u001b[1mcontainer\u001b[m\n",
            "        Deploy and manage clusters of machines for running containers.\n",
            "\n",
            "     \u001b[1mdataflow\u001b[m\n",
            "        Manage Google Cloud Dataflow jobs.\n",
            "\n",
            "     \u001b[1mdataproc\u001b[m\n",
            "        Create and manage Google Cloud Dataproc clusters and jobs.\n",
            "\n",
            "     \u001b[1mdatastore\u001b[m\n",
            "        Manage your Cloud Datastore indexes.\n",
            "\n",
            "     \u001b[1mdebug\u001b[m\n",
            "        Commands for interacting with the Cloud Debugger.\n",
            "\n",
            "     \u001b[1mdeployment-manager\u001b[m\n",
            "        Manage deployments of cloud resources.\n",
            "\n",
            "     \u001b[1mdns\u001b[m\n",
            "        Manage your Cloud DNS managed-zones and record-sets.\n",
            "\n",
            "     \u001b[1mdomains\u001b[m\n",
            "        Manage domains for your Google Cloud projects.\n",
            "\n",
            "     \u001b[1mendpoints\u001b[m\n",
            "        Create, enable and manage API services.\n",
            "\n",
            "     \u001b[1mfilestore\u001b[m\n",
            "        Create and manipulate Cloud Filestore resources.\n",
            "\n",
            "     \u001b[1mfirebase\u001b[m\n",
            "        Work with Google Firebase.\n",
            "\n",
            "     \u001b[1mfunctions\u001b[m\n",
            "        Manage Google Cloud Functions.\n",
            "\n",
            "     \u001b[1miam\u001b[m\n",
            "        Manage IAM service accounts and keys.\n",
            "\n",
            "     \u001b[1miot\u001b[m\n",
            "        Manage Cloud IoT resources.\n",
            "\n",
            "     \u001b[1mkms\u001b[m\n",
            "        Manage cryptographic keys in the cloud.\n",
            "\n",
            "     \u001b[1mlogging\u001b[m\n",
            "        Manage Stackdriver Logging.\n",
            "\n",
            "     \u001b[1mml\u001b[m\n",
            "        Use Google Cloud machine learning capabilities.\n",
            "\n",
            "     \u001b[1mml-engine\u001b[m\n",
            "        Manage AI Platform jobs and models.\n",
            "\n",
            "     \u001b[1morganizations\u001b[m\n",
            "        Create and manage Google Cloud Platform Organizations.\n",
            "\n",
            "     \u001b[1mprojects\u001b[m\n",
            "        Create and manage project access policies.\n",
            "\n",
            "     \u001b[1mpubsub\u001b[m\n",
            "        Manage Cloud Pub/Sub topics, subscriptions, and snapshots.\n",
            "\n",
            "     \u001b[1mredis\u001b[m\n",
            "        Manage Cloud Memorystore Redis resources.\n",
            "\n",
            "     \u001b[1mresource-manager\u001b[m\n",
            "        Manage Cloud Resources.\n",
            "\n",
            "     \u001b[1mscheduler\u001b[m\n",
            "        Manage Cloud Scheduler jobs and schedules.\n",
            "\n",
            "     \u001b[1mservices\u001b[m\n",
            "        List, enable and disable APIs and services.\n",
            "\n",
            "     \u001b[1msource\u001b[m\n",
            "        Cloud git repository commands.\n",
            "\n",
            "     \u001b[1mspanner\u001b[m\n",
            "        Command groups for Cloud Spanner.\n",
            "\n",
            "     \u001b[1msql\u001b[m\n",
            "        Create and manage Google Cloud SQL databases.\n",
            "\n",
            "     \u001b[1mtasks\u001b[m\n",
            "        Manage Cloud Tasks queues and tasks.\n",
            "\n",
            "     \u001b[1mtopic\u001b[m\n",
            "        gcloud supplementary help.\n",
            "\n",
            "\u001b[m\u001b[1mCOMMANDS\u001b[m\n",
            "    \u001b[1m\u001b[1;4mCOMMAND\u001b[1m\u001b[m is one of the following:\n",
            "\n",
            "     \u001b[1mdocker\u001b[m\n",
            "        \u001b[1m(DEPRECATED)\u001b[m Enable Docker CLI access to Google Container Registry.\n",
            "\n",
            "     \u001b[1mfeedback\u001b[m\n",
            "        Provide feedback to the Google Cloud SDK team.\n",
            "\n",
            "     \u001b[1mhelp\u001b[m\n",
            "        Search gcloud help text.\n",
            "\n",
            "     \u001b[1minfo\u001b[m\n",
            "        Display information about the current gcloud environment.\n",
            "\n",
            "     \u001b[1minit\u001b[m\n",
            "        Initialize or reinitialize gcloud.\n",
            "\n",
            "     \u001b[1mversion\u001b[m\n",
            "        Print version information for Cloud SDK components.\n",
            "\u001b[m"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czVwgSB-WaqA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mv_a__YlWef5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}